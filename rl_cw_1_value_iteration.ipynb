{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0fbbb4c579309cc8ef75c0e632529bc7",
     "grade": false,
     "grade_id": "cell-33b0e4dce2016840",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# CM50270 Reinforcement Learning\n",
    "## Coursework Part 1: Value Iteration\n",
    "\n",
    "In this exercise, you will implement the value iteration algorithm for three closely related, but different, gridworld environments.\n",
    "\n",
    "**Total number of marks:** 20 marks.\n",
    "\n",
    "**What to submit:** Your completed Jupyter notebook (.ipynb file) which should include **all** of your source code. Please **do not change the file name or compress/zip your submission**. Please do not include any identifying information on the files you submit. This coursework will be marked **anonymously**.\n",
    "\n",
    "**Where to submit:** CM50270 Moodle Page.\n",
    "\n",
    "You are required to **work individually**. You are welcome to discuss ideas with others but you must design your own implementation and **write your own code**.\n",
    "\n",
    "**Do not plagiarise**. Plagiarism is a serious academic offence. For details on what plagiarism is and how to avoid it, please visit the following webpage: http://www.bath.ac.uk/library/help/infoguides/plagiarism.html\n",
    "\n",
    "If you are asked to use specific variable names, data-types, function signatures and notebook cells, **please ensure that you follow these instructions**. Not doing so will cause the automarker to reject your work, and will assign you a score of zero for that question. **If the automarker rejects your work because you have not followed the instructions, you may not get any credit for your work**.\n",
    "\n",
    "Please **do not use any non-standard, third-party libraries** apart from numpy and matplotlib. **If we are unable to run your code because you have used unsupported external libraries, you may not get any credit for your work.**\n",
    "\n",
    "Please remember to **save your work regularly**.\n",
    "\n",
    "Please be sure to **restart the kernel and run your code from start-to-finish** (Kernel â†’ Restart & Run All) before submitting your notebook. Otherwise, you may not be aware that you are using variables in memory that you have deleted.\n",
    "\n",
    "Your total runtime must be less than **1 minute** on the University's computers. Otherwise, you may not get credit for your work. You can run your code on the university's computers remotely using [UniDesk](https://bath.topdesk.net/tas/public/ssp/content/detail/knowledgeitem?unid=ff3266344c1d4eb2acb227cc9e3e1eee)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1b1f63de0fdcf845091a97902c847d2e",
     "grade": false,
     "grade_id": "cell-49e38b6d0da7d1fa",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Introduction\n",
    "In this coursework, you will implement the Value Iteration algorithm to compute an optimal policy for three different (but closely related) Markov Decision Processes. For your reference, the pseudo-code for the Value Iteration algorithm is reproduced below from the textbook (Reinforcement Learning, Sutton & Barto, 2018, pp. 83).\n",
    "\n",
    "<img src=\"images/value_iteration.png\" style=\"width: 800px;\"/>\n",
    "\n",
    "Please note the following about the pseudo-code: The set $\\mathcal{S}$ contains all non-terminal states, whereas $\\mathcal{S}^+$ is the set of all states (terminal and non-terminal). The symbol $r$ represents the immediate reward on transition from state $s$ to the next state $s'$ via action $a$. \n",
    "\n",
    "<img src=\"images/bombs and gold numbers.png\" style=\"width: 300px;\" align=\"left\" caption=\"Figure 1\"/>\n",
    "\n",
    "The three problems you will solve use variants of the gridworld environment shown on the left. You should be familiar with this kind of environment from the lectures. The grid squares in the figure are numbered as shown. In all exercises, the following are true: \n",
    "\n",
    "**Actions available:** The agent has four possible actions in each grid square. These are *west*, *north*, *south*, and *east*. If the direction of movement is blocked by a wall (for example, if the agent executes action south at grid square 1), the agent remains in the same grid square. \n",
    "\n",
    "**Collecting gold:** On its first arrival at a grid square that contains gold (from a neighbouring grid square), the agent collects the gold. Note that, in order to collect the gold, the agent needs to transition into the grid square (containing the gold) from a different grid square.\n",
    "\n",
    "**Hitting the bomb:** On arrival at a grid square that contains a bomb (from a neighbouring grid square), the agent activates the bomb. \n",
    "\n",
    "**Terminal states:** The game terminates when all gold is collected or when the bomb is activated. In Exercises 1 and 2, you can define terminal states to be grid squares 18 and 23. In Exercise 3, you will need to define terminal state(s) differently.\n",
    "\n",
    "\n",
    "### Instructions ###\n",
    "Set parameter $\\theta$ to $1 \\times 10 ^{-10}$. You can express that as `1e-10` in Python. \n",
    "\n",
    "Set all initial state values $V(s)$ to zero.\n",
    "\n",
    "Do not use discounting (that is, set $\\gamma=1$).\n",
    "\n",
    "Use the following reward function: $-1$ for each navigation action (including when the action results in hitting the wall), an additional $+10$ for collecting each piece of gold, and an additional $-10$ for activating the bomb. For example, the immediate reward for transitioning into a square with gold (from a neighbouring grid square) is $-1 + 10 = +9$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a42c1c34d1fb04bef22da8276ff9780c",
     "grade": false,
     "grade_id": "cell-bb45c706447879a3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Exercise 1: Deterministic Environment (0 Marks)\n",
    "\n",
    "In this exercise, the agent is able to move in the intended direction with certainty. For example, if it executes action _north_ in grid square 0, it will transition to grid square 5 with probability 1. In other words, we have a deterministic environment.\n",
    "\n",
    "Compute the optimal policy using Value Iteration. \n",
    "\n",
    "Your need to produce two one-dimensional numpy arrays with names `policy` and `v`. Both arrays should have a length of 25, with the element at index $i$ representing grid cell $i$ (see figure above). Both arrays should be accessible in the \"solution cell\" below!\n",
    "\n",
    "The array `policy` should be a numpy array of strings that specifies an optimal action at each grid location. Please use the abbreviations `\"n\"`, `\"e\"`, `\"s\"`, and `\"w\"` for the four actions. As an example, the value of `policy` at index `0` needs to give `\"n\"`, if _north_ is an optimal action in cell 0. The policy for a terminal state can be any action. If there are multiple optimal actions from a state, any optimal action will be considered as a correct answer. \n",
    "\n",
    "The array `v` should be an array of floats that contains the expected return at each grid square (that is, the state value under the optimal policy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ea5b17f9f7aea9b1f962fc29862764bc",
     "grade": false,
     "grade_id": "cw1_value_iteration_deterministic",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Please write your code for Exercise 1 in this cell or in as many cells as you want ABOVE this cell.\n",
    "# Your code should compute the values of policy and v from scratch when this cell is executed, \n",
    "# using the value iteration algorithm. We will mark your coursework by checking the values of \n",
    "# the variables policy and v in the hidden test cell further below. Do NOT delete this cell.\n",
    "\n",
    "# YOUR CODE HERE\n",
    "import numpy as np\n",
    "\n",
    "class Gridworld_Det():\n",
    "    def __init__(self,rows=5,coloumns=5,bombpos=[18],goldpos=[23]):\n",
    "        self.num_r = rows\n",
    "        self.num_c = coloumns\n",
    "        self.shape = (self.num_r*self.num_c)\n",
    "        self.bombpos = bombpos\n",
    "        self.goldpos = goldpos\n",
    "        self.num_fields = range(self.num_r * self.num_c)\n",
    "        self.gold_reward = 10\n",
    "        self.bomb_reward = -10\n",
    "        self.terminalstates = self.bombpos + self.goldpos\n",
    "        self.actionreward = -1\n",
    "        self.actions = [\"n\", \"e\", \"s\", \"w\"]\n",
    "        self.num_actions = len(self.actions)\n",
    "        self.reward = np.zeros(shape=(self.shape))\n",
    "        self.val = np.zeros(shape=(self.shape))\n",
    "        self.policy = np.empty(25, dtype=np.unicode)\n",
    "        self.gamma = 1\n",
    "        self.theta = 1e-10\n",
    "        \n",
    "    def Reward_propagation(self, starting_pos, action):\n",
    "        new_position = starting_pos\n",
    "        reward = 0\n",
    "        if starting_pos in self.terminalstates:\n",
    "            return starting_pos,0\n",
    "        else:\n",
    "            if action == \"n\":\n",
    "                candidate_pos = starting_pos + self.num_c\n",
    "                new_position = starting_pos if starting_pos + 5 >= self.shape else candidate_pos\n",
    "                reward = self.actionreward\n",
    "            elif action == \"e\":\n",
    "                candidate_pos = starting_pos + 1 \n",
    "                new_position = starting_pos if (starting_pos + 1) % 5 == 0 else candidate_pos\n",
    "                reward = self.actionreward\n",
    "            elif action == \"s\":\n",
    "                candidate_pos = starting_pos - self.num_c\n",
    "                new_position = starting_pos if starting_pos - 5 < 0 else candidate_pos\n",
    "                reward = self.actionreward\n",
    "            elif action == \"w\":\n",
    "                candiate_pos = starting_pos - 1\n",
    "                new_position = starting_pos if (starting_pos) % 5 == 0 else candiate_pos\n",
    "                reward = self.actionreward\n",
    "            if new_position == self.goldpos[0]:\n",
    "                reward += self.gold_reward \n",
    "            elif new_position == self.bombpos[0]:\n",
    "                reward += self.bomb_reward \n",
    "        return new_position, reward\n",
    "        \n",
    "        \n",
    "    def ValueIteration(self):\n",
    "        while True:\n",
    "            delta = 0\n",
    "            prev = self.val.copy()\n",
    "            for state in self.num_fields:\n",
    "                new_reward=np.zeros(4)\n",
    "                #for act in range(len(self.actions)):\n",
    "                for i,act in enumerate(self.actions):\n",
    "                    new_position,possible_reward = self.Reward_propagation(state,act)\n",
    "                    new_reward[i] += 1*(possible_reward+self.gamma*self.val[new_position])\n",
    "                self.val[state] = max(new_reward)\n",
    "            delta = np.fabs(self.val-prev).max()\n",
    "           # print(delta)\n",
    "            if delta < self.theta:\n",
    "                break\n",
    "        return self.val\n",
    "        \n",
    "    def PolicyIteration(self):\n",
    "        prev = self.val.copy()\n",
    "        for state in self.num_fields:\n",
    "            Policy_reward = []\n",
    "            #print(Policy_reward.dtype)\n",
    "            for act in self.actions:\n",
    "                new_position,possible_reward = self.Reward_propagation(state,act)\n",
    "                Policy_reward.append(possible_reward+self.gamma*self.val[new_position])\n",
    "            Policy_reward = np.array(Policy_reward)\n",
    "            Exp_reward = np.argmax(Policy_reward)\n",
    "            self.policy[state] = self.actions[Exp_reward]\n",
    "        return self.policy\n",
    "   \n",
    "                \n",
    "                \n",
    "            \n",
    "        \n",
    "        \n",
    "environment = Gridworld_Det()\n",
    "\n",
    "\n",
    "\n",
    "v = environment.ValueIteration()\n",
    "policy = environment.PolicyIteration()\n",
    "\n",
    "#policy = print(environment.PolicyIteration().reshape(5,5))\n",
    "#print(environment.action_reward(10,3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3f6a967ae761f92bf2a79af2be971939",
     "grade": false,
     "grade_id": "cell-02a5c34a5b828d1c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Example Test Cell**\n",
    "\n",
    "In the code cell below, we have provided an example of the type of test code that we will use to mark your work. In these tests, we first check that your `policy` and `v` variables are of the correct type, and then check that their values match the solution. In the future, the test code will be hidden from you.\n",
    "\n",
    "You must not delete or modify test cells in any way - any modifications you do make will be overwritten at run-time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5e11d89ae67ea8af3e01f18e48bd35f6",
     "grade": true,
     "grade_id": "cw1_value_iteration_deterministic_tests",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Student's policy:\n",
      "[['e' 'e' 'e' 'n' 'w']\n",
      " ['n' 'n' 'n' 'n' 'n']\n",
      " ['n' 'n' 'n' 'e' 'n']\n",
      " ['n' 'n' 'n' 'n' 'n']\n",
      " ['n' 'n' 'n' 'n' 'n']]\n",
      "Solution policy:\n",
      "[['e' 'e' 'e' 'n' 'w']\n",
      " ['n' 'n' 'n' 'n' 'n']\n",
      " ['n' 'n' 'n' 'e' 'n']\n",
      " ['n' 'n' 'n' 'n' 'n']\n",
      " ['n' 'n' 'n' 'n' 'n']]\n",
      "Student's v:\n",
      "[[7. 8. 9. 0. 9.]\n",
      " [6. 7. 8. 0. 8.]\n",
      " [5. 6. 7. 6. 7.]\n",
      " [4. 5. 6. 5. 6.]\n",
      " [3. 4. 5. 4. 5.]]\n",
      "Solution v:\n",
      "[[7. 8. 9. 0. 9.]\n",
      " [6. 7. 8. 0. 8.]\n",
      " [5. 6. 7. 6. 7.]\n",
      " [4. 5. 6. 5. 6.]\n",
      " [3. 4. 5. 4. 5.]]\n"
     ]
    }
   ],
   "source": [
    "# DO NOT DELETE OR MODIFY THIS CELL!\n",
    "# Your code for Exercise 1 is tested here.\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# We're giving you the solution values for Exercise 1, but not telling you how to compute them!\n",
    "solution_values = [3.0, 4.0, 5.0, 4.0, 5.0,\n",
    "                   4.0, 5.0, 6.0, 5.0, 6.0,\n",
    "                   5.0, 6.0, 7.0, 6.0, 7.0,\n",
    "                   6.0, 7.0, 8.0, 0.0, 8.0,\n",
    "                   7.0, 8.0, 9.0, 0.0, 9.0]\n",
    "solution_values = np.array(solution_values)\n",
    "\n",
    "# We're giving you the solution policy for Exercise 1, but not telling you how to compute it!\n",
    "solution_policy = [\n",
    "                   'n', 'n', 'n', 'n', 'n',\n",
    "                   'n', 'n', 'n', 'n', 'n',\n",
    "                   'n', 'n', 'n', 'e', 'n',\n",
    "                   'n', 'n', 'n', 'n', 'n',\n",
    "                   'e', 'e', 'e', 'n', 'w',]\n",
    "solution_policy = np.array(solution_policy)\n",
    "\n",
    "# Check that policy and v are numpy arrays.\n",
    "assert(isinstance(policy, np.ndarray))\n",
    "assert(isinstance(v, np.ndarray))\n",
    "\n",
    "# Check correct shapes of numpy arrays.\n",
    "assert(policy.shape == (25, ))\n",
    "assert(v.shape == (25, ))\n",
    "\n",
    "# Check whether the numpy arrays have the correct data types.\n",
    "assert(np.issubdtype(policy.dtype, np.unicode_)) # policy.dtype should be '<U1'\n",
    "assert(np.issubdtype(v.dtype, np.float64))\n",
    "\n",
    "# Check whether policy contains only \"n\", \"w\", \"s\", or \"e\" values.\n",
    "assert(np.all(np.isin(policy, np.array([\"n\", \"w\", \"s\", \"e\"]))))\n",
    "\n",
    "# Print student's solution and true solution for easier comparison / spotting of errors.\n",
    "print(\"Student's policy:\")\n",
    "print(np.flip(policy.reshape((5, 5)), 0))\n",
    "print(\"Solution policy:\")\n",
    "print(np.flip(solution_policy.reshape((5, 5)), 0))\n",
    "\n",
    "print(\"Student's v:\")\n",
    "print(np.flip(v.reshape((5, 5)), 0))\n",
    "print(\"Solution v:\")\n",
    "print(np.flip(solution_values.reshape((5, 5)), 0))\n",
    "\n",
    "# Compare policy (only on states that have a single optimal direction).\n",
    "states_to_check =  np.array([4, 9, 14, 17, 19, 20, 22, 24])\n",
    "np.testing.assert_array_equal(policy[states_to_check], solution_policy[states_to_check])\n",
    "\n",
    "# Compare state_values (also for terminal states --- they have to be zero!).\n",
    "states_to_check = np.delete(np.arange(25), np.array([18, 23]))\n",
    "np.testing.assert_array_almost_equal(v[states_to_check], solution_values[states_to_check], decimal=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "db00038a44803605ff0fe1f8e0971696",
     "grade": false,
     "grade_id": "cell-05eb78b7446cb694",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Exercise 2: Stochastic Environment (12 Marks)\n",
    "\n",
    "In this exercise, we introduce stochasticity into the environment. Now, the agent is not always able to execute its actions as intended.\n",
    "\n",
    "With probability 0.8, the agent moves as intended. However, with probability 0.2, it moves in a random direction.\n",
    "\n",
    "For example, from grid square 0, if the agent tries to move north, with probability 0.8 the action will work as intended. But with probability 0.2, the agent's motor control system will move it in a random direction (including north). So, it will randomly try to move west, east, north or south with probability 0.05 each. Notice that the total probability of moving to square 5 (as intended) is 0.8 + 0.05 = 0.85.\n",
    " \n",
    "Compute the optimal policy using Value Iteration.\n",
    "\n",
    "Your value iteration method should output two one-dimensional numpy arrays with names `policy` and `v`, as in Exercise 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "596c020f7ef0ae864054c8dbc3496cc2",
     "grade": false,
     "grade_id": "cw1_value_iteration_stochastic",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6.04169329 7.28756636 8.61359951 0.         8.69262311]\n",
      " [4.86185111 5.99087587 6.37082431 0.         6.46721593]\n",
      " [3.67550938 4.69621388 4.99441863 3.2189158  5.10250988]\n",
      " [2.48699534 3.40945989 3.66922967 2.64122933 3.78610115]\n",
      " [1.35979208 2.19733672 2.42878751 1.57272161 2.55202451]]\n",
      "[['e' 'e' 'e' 'n' 'w']\n",
      " ['n' 'n' 'n' 'n' 'n']\n",
      " ['n' 'n' 'n' 'e' 'n']\n",
      " ['n' 'n' 'n' 'e' 'n']\n",
      " ['n' 'n' 'n' 'n' 'n']]\n"
     ]
    }
   ],
   "source": [
    "# Please write your code for Exercise 2 in this cell or in as many cells as you want ABOVE this cell.\n",
    "# Your code should compute the values of policy and v from scratch when this cell is executed, \n",
    "# using the value iteration algorithm. We will mark your coursework by checking the values of \n",
    "# the variables policy and v in the hidden test cell further below. Do NOT delete this cell.\n",
    "\n",
    "# YOUR CODE HERE\n",
    "\n",
    "# YOUR CODE HERE\n",
    "import numpy as np\n",
    "\n",
    "class Gridworld_Stoch():\n",
    "    def __init__(self,rows=5,coloumns=5,bombpos=[18],goldpos=[23]):\n",
    "        self.num_r = rows\n",
    "        self.num_c = coloumns\n",
    "        self.shape = (self.num_r*self.num_c)\n",
    "        self.bombpos = bombpos\n",
    "        self.goldpos = goldpos\n",
    "        self.num_fields = range(self.num_r * self.num_c)\n",
    "        self.gold_reward = 10\n",
    "        self.bomb_reward = -10\n",
    "        self.terminalstates = self.bombpos + self.goldpos\n",
    "        self.actionreward = -1\n",
    "        self.actions = [\"n\", \"e\", \"s\", \"w\"]\n",
    "        self.num_actions = len(self.actions)\n",
    "        self.reward = np.zeros(shape=(self.shape))\n",
    "        self.val = np.zeros(shape=(self.shape))\n",
    "        self.policy = np.empty(25, dtype=np.unicode)\n",
    "        self.gamma = 1\n",
    "        self.theta = 1e-10\n",
    "        \n",
    "    def Reward_propagation(self, starting_pos, action):\n",
    "        new_position = starting_pos\n",
    "        reward = 0\n",
    "        if starting_pos in self.terminalstates:\n",
    "            return starting_pos,0\n",
    "        else:\n",
    "            if action == \"n\":\n",
    "                candidate_pos = starting_pos + self.num_c\n",
    "                new_position = starting_pos if starting_pos + 5 >= self.shape else candidate_pos\n",
    "                reward = self.actionreward\n",
    "            elif action == \"e\":\n",
    "                candidate_pos = starting_pos + 1 \n",
    "                new_position = starting_pos if (starting_pos + 1) % 5 == 0 else candidate_pos\n",
    "                reward = self.actionreward\n",
    "            elif action == \"s\":\n",
    "                candidate_pos = starting_pos - self.num_c\n",
    "                new_position = starting_pos if starting_pos - 5 < 0 else candidate_pos\n",
    "                reward = self.actionreward\n",
    "            elif action == \"w\":\n",
    "                candiate_pos = starting_pos - 1\n",
    "                new_position = starting_pos if (starting_pos) % 5 == 0 else candiate_pos\n",
    "                reward = self.actionreward\n",
    "            if new_position == self.goldpos[0]:\n",
    "                reward += self.gold_reward \n",
    "            elif new_position == self.bombpos[0]:\n",
    "                reward += self.bomb_reward \n",
    "        return new_position, reward\n",
    "        \n",
    "#     def ValueIteration(self):\n",
    "#         while True:\n",
    "#             deta = 0\n",
    "#             prev = self.val.copy()\n",
    "#             for state in self.num_fields:\n",
    "#                 new_reward=[0,0,0,0]\n",
    "#                 for i,act in enumerate(self.actions):\n",
    "#                     if act == \"n\":             \n",
    "#                         new_position,possible_reward = self.Reward_propagation(state,\"e\")\n",
    "#                         new_reward[i] += 0.05*(possible_reward+self.gamma*self.val[new_position])\n",
    "#                         new_position,possible_reward = self.Reward_propagation(state,\"s\")\n",
    "#                         new_reward[i] += 0.05*(possible_reward+self.gamma*self.val[new_position])\n",
    "#                         new_position,possible_reward = self.Reward_propagation(state,3)\n",
    "#                         new_reward[i] += 0.05*(possible_reward+self.gamma*self.val[new_position])\n",
    "#                         new_position,possible_reward = self.Reward_propagation(state,\"act\")\n",
    "#                         new_reward[i] += 0.85*(possible_reward+self.gamma*self.val[new_position])\n",
    "#                     if act == \"e\":             \n",
    "#                         new_position,possible_reward = self.Reward_propagation(state,act)\n",
    "#                         new_reward[i] += 0.85*(possible_reward+self.gamma*self.val[new_position])\n",
    "#                         new_position,possible_reward = self.Reward_propagation(state,\"s\")\n",
    "#                         new_reward[i] += 0.05*(possible_reward+self.gamma*self.val[new_position])\n",
    "#                         new_position,possible_reward = self.Reward_propagation(state,3)\n",
    "#                         new_reward[i] += 0.05*(possible_reward+self.gamma*self.val[new_position])\n",
    "#                         new_position,possible_reward = self.Reward_propagation(state,\"n\")\n",
    "#                         new_reward[i] += 0.05*(possible_reward+self.gamma*self.val[new_position])\n",
    "#                     if act == \"s\":             \n",
    "#                         new_position,possible_reward = self.Reward_propagation(state,\"n\")\n",
    "#                         new_reward[i] += 0.05*(possible_reward+self.gamma*self.val[new_position])\n",
    "#                         new_position,possible_reward = self.Reward_propagation(state,\"e\")\n",
    "#                         new_reward[i] += 0.05*(possible_reward+self.gamma*self.val[new_position])\n",
    "#                         new_position,possible_reward = self.Reward_propagation(state,act)\n",
    "#                         new_reward[i] += 0.85*(possible_reward+self.gamma*self.val[new_position])\n",
    "#                         new_position,possible_reward = self.Reward_propagation(state,3)\n",
    "#                         new_reward[i] += 0.05*(possible_reward+self.gamma*self.val[new_position])\n",
    "#                     if act == \"w\":\n",
    "#                         new_position,possible_reward = self.Reward_propagation(state,\"n\")\n",
    "#                         new_reward[i] += 0.05*(possible_reward+self.gamma*self.val[new_position])\n",
    "#                         new_position,possible_reward = self.Reward_propagation(state,\"e\")\n",
    "#                         new_reward[i] += 0.05*(possible_reward+self.gamma*self.val[new_position])\n",
    "#                         new_position,possible_reward = self.Reward_propagation(state,\"s\")\n",
    "#                         new_reward[i] += 0.05*(possible_reward+self.gamma*self.val[new_position])\n",
    "#                         new_position,possible_reward = self.Reward_propagation(state,act)\n",
    "#                         new_reward[i] += 0.85*(possible_reward+self.gamma*self.val[new_position])\n",
    "#                 self.val[state] = max(new_reward)\n",
    "#             delta = np.fabs(self.val-prev).max()\n",
    "#            # print(delta)\n",
    "#             if delta < self.theta:\n",
    "#                 break\n",
    "#         return self.val\n",
    "\n",
    "        \n",
    "    def ValueIteration(self):\n",
    "        while True:\n",
    "            delta = 0\n",
    "            prev = self.val.copy()\n",
    "            for state in self.num_fields:\n",
    "                new_reward = np.zeros(4)\n",
    "#check if position n,e,s,w with probabilities of stochasic environment and add this to value table during propagation\n",
    "                for i,act in enumerate(self.actions):\n",
    "                    for j,act in enumerate(self.actions):\n",
    "                        if j!=i:\n",
    "                            new_position, possible_reward = self.Reward_propagation(state,act)\n",
    "                            new_reward[i] += 0.05*(possible_reward+self.gamma*self.val[new_position])\n",
    "                        else:\n",
    "                            new_position, possible_reward = self.Reward_propagation(state,act)\n",
    "                            new_reward[i] += 0.85*(possible_reward+self.gamma*self.val[new_position])\n",
    "                self.val[state] = max(new_reward)\n",
    "            delta = np.fabs(self.val-prev).max()\n",
    "            #print(delta)\n",
    "            if delta < self.theta:\n",
    "                break\n",
    "        return self.val\n",
    "            \n",
    "                        \n",
    "                \n",
    "    \n",
    "    \n",
    "    def PolicyIteration(self):\n",
    "        prev = self.val.copy()\n",
    "        for state in self.num_fields:\n",
    "            Policy_reward = []\n",
    "            #print(Policy_reward.dtype)\n",
    "            for act in self.actions:\n",
    "                new_position,possible_reward = self.Reward_propagation(state,act)\n",
    "                Policy_reward.append(possible_reward+self.gamma*self.val[new_position])\n",
    "            Policy_reward = np.array(Policy_reward)\n",
    "            Exp_reward = np.argmax(Policy_reward)\n",
    "            self.policy[state] = self.actions[Exp_reward]\n",
    "        return self.policy\n",
    "   \n",
    "    \n",
    "    \n",
    "    \n",
    "environment = Gridworld_Stoch()\n",
    "\n",
    "v = (environment.ValueIteration())\n",
    "print(np.flip(v.reshape((5, 5)), 0))\n",
    "policy = environment.PolicyIteration()\n",
    "print(np.flip(policy.reshape((5, 5)), 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f0152b199697503339f0fd7f0f935472",
     "grade": true,
     "grade_id": "cw1_value_iteration_stochastic_tests",
     "locked": true,
     "points": 12,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# DO NOT DELETE OR MODIFY THIS CELL!\n",
    "# Your code for Exercise 2 is tested here.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f8fb50a4a7886296d52685a540d2c162",
     "grade": false,
     "grade_id": "cell-e0de56802818cf9b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Exercise 3: Stochastic Environment with Two Pieces of Gold (8 marks)\n",
    "\n",
    "<img src=\"images/bomb and two gold.png\" style=\"width: 300px;\" align=\"left\" caption=\"Figure 1\"/> In this exercise, we have modified the stochastic environment presented in exercise 2. A second piece of gold has been placed on grid square 12. The terminal state is reached only when **all** pieces of gold are collected or when the bomb is activated.\n",
    "\n",
    "Compute the optimal policy for this altered environment using Value Iteration.\n",
    "\n",
    "Hint: You will need to change your state representation in order to account for the additional piece of gold.\n",
    "\n",
    "Your method should output two one-dimensional numpy arrays with names `policy` and `v`, as in the previous exercises. These arrays should specify the expected return and an optimal policy at the corresponding grid sqaure **before any pieces of gold are collected or a bomb is activated.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "635f8795634a99a01b6cc3e5f28ecb3a",
     "grade": false,
     "grade_id": "cw1_value_iteration_two_gold",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[10.65103994 11.79603433 13.00848756  4.28547547 12.97048714]\n",
      " [11.1861353  12.32932372 12.5121464   0.         10.61568556]\n",
      " [12.28702748 13.59365638  4.99441863 12.41930416 11.19974428]\n",
      " [11.17522837 12.35165962 13.59092292 12.28122217 11.051285  ]\n",
      " [10.06409806 11.17488271 12.28045984 11.10816476  9.99389366]]\n",
      "[['e' 'e' 'e' 'n' 'w']\n",
      " ['e' 's' 's' 'n' 'n']\n",
      " ['e' 'e' 'n' 'w' 'w']\n",
      " ['e' 'n' 'n' 'w' 'w']\n",
      " ['n' 'n' 'n' 'n' 'w']]\n"
     ]
    }
   ],
   "source": [
    "# Please write your code for Exercise 3 in this cell or in as many cells as you want ABOVE this cell.\n",
    "# Your code should compute the values of policy and v from scratch when this cell is executed, \n",
    "# using the value iteration algorithm. We will mark your coursework by checking the values of \n",
    "# the variables policy and v in the hidden test cell further below. Do NOT delete this cell.\n",
    "\n",
    "# YOUR CODE HERE\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class Gridworld_Stoc_gold():\n",
    "    def __init__(self,rows=5,coloumns=5,bombpos=[18],goldpos=[12,23]):\n",
    "        self.num_r = rows\n",
    "        self.num_c = coloumns\n",
    "        self.shape = (self.num_r*self.num_c)\n",
    "        self.bombpos = bombpos\n",
    "        self.goldpos = goldpos\n",
    "        self.num_fields = range(self.num_r * self.num_c)\n",
    "        self.gold_reward = 10\n",
    "        self.bomb_reward = -10\n",
    "        self.terminalstates = self.bombpos + self.goldpos\n",
    "        self.actionreward = -1\n",
    "        self.actions = [\"n\", \"e\", \"s\", \"w\"]\n",
    "        self.num_actions = len(self.actions)\n",
    "        self.reward = np.zeros(shape=(self.shape))\n",
    "        self.val = np.zeros(shape=(self.shape))\n",
    "        self.policy = np.empty(25, dtype=np.unicode)\n",
    "        self.gamma = 1\n",
    "        self.theta = 1e-10\n",
    "        \n",
    "    def Reward_propagation(self, starting_pos, action):\n",
    "        new_position = starting_pos\n",
    "        reward = 0\n",
    "        if starting_pos in self.terminalstates:\n",
    "            return starting_pos,0\n",
    "        else:\n",
    "            if action == \"n\":\n",
    "                candidate_pos = starting_pos + self.num_c\n",
    "                new_position = starting_pos if starting_pos + 5 >= self.shape else candidate_pos\n",
    "                reward = self.actionreward\n",
    "            elif action == \"e\":\n",
    "                candidate_pos = starting_pos + 1 \n",
    "                new_position = starting_pos if (starting_pos + 1) % 5 == 0 else candidate_pos\n",
    "                reward = self.actionreward\n",
    "            elif action == \"s\":\n",
    "                candidate_pos = starting_pos - self.num_c\n",
    "                new_position = starting_pos if starting_pos - 5 < 0 else candidate_pos\n",
    "                reward = self.actionreward\n",
    "            elif action == \"w\":\n",
    "                candiate_pos = starting_pos - 1\n",
    "                new_position = starting_pos if (starting_pos) % 5 == 0 else candiate_pos\n",
    "                reward = self.actionreward\n",
    "            if (new_position in self.goldpos): #check that new position is in the either of the gold positions\n",
    "                reward += self.gold_reward \n",
    "            elif new_position == self.bombpos[0]:\n",
    "                reward += self.bomb_reward \n",
    "        return new_position, reward\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "   \n",
    "    def ValueIteration(self):\n",
    "        while True:\n",
    "            delta = 0\n",
    "            prev = self.val.copy()\n",
    "            for state in self.num_fields:\n",
    "                \n",
    "#                 check if state is in gold position 1 or 2 and call environment at this state recurvisley to continue to a terminal state\n",
    "#                 track the state in value table and continue \n",
    "\n",
    "                if state in self.goldpos: \n",
    "                    if len(self.goldpos) > 1:\n",
    "                        environment = Gridworld_Stoc_gold(self.num_r,self.num_c,self.bombpos,[self.goldpos[0] if state != self.goldpos[0] else self.goldpos[1]])\n",
    "                        self.val[state] = environment.ValueIteration()[state] \n",
    "                        #print(self.val)\n",
    "\n",
    "\n",
    "                    \n",
    "                else:\n",
    "                    new_reward = np.zeros(4)\n",
    "                    for i,act in enumerate(self.actions):\n",
    "                        \n",
    "                        for j,act in enumerate(self.actions):\n",
    "                            if j!=i:\n",
    "                                new_position, possible_reward = self.Reward_propagation(state,act)\n",
    "                                new_reward[i] += 0.05*(possible_reward+self.gamma*self.val[new_position])\n",
    "                                \n",
    "                            else:\n",
    "                                new_position, possible_reward = self.Reward_propagation(state,act)\n",
    "                                new_reward[i] += 0.85*(possible_reward+self.gamma*self.val[new_position])\n",
    "                    self.val[state] = max(new_reward)\n",
    "#                     print(self.val)\n",
    "            delta = np.fabs(self.val-prev).max()\n",
    "            #print(delta)\n",
    "            if delta < self.theta:\n",
    "                break\n",
    "        return self.val\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "                        \n",
    "            \n",
    "       \n",
    "    \n",
    "    \n",
    "    def PolicyIteration(self):\n",
    "        prev = self.val.copy()\n",
    "        for state in self.num_fields:\n",
    "            Policy_reward = []\n",
    "            for act in self.actions:\n",
    "                new_position,possible_reward = self.Reward_propagation(state,act)\n",
    "                Policy_reward.append(possible_reward+self.gamma*self.val[new_position])\n",
    "            Policy_reward = np.array(Policy_reward)\n",
    "            Exp_reward = np.argmax(Policy_reward)\n",
    "            self.policy[state] = self.actions[Exp_reward]\n",
    "        return self.policy\n",
    "   \n",
    "    \n",
    "    \n",
    "    \n",
    "environment = Gridworld_Stoc_gold()\n",
    "\n",
    "v = (environment.ValueIteration())\n",
    "print(np.flip(v.reshape((5, 5)), 0))\n",
    "policy = environment.PolicyIteration()\n",
    "print(np.flip(policy.reshape((5, 5)), 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cf587ef9d694513182a8ca1c721200d7",
     "grade": true,
     "grade_id": "cw1_value_iteration_two_gold_tests",
     "locked": true,
     "points": 8,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# DO NOT DELETE OR MODIFY THIS CELL!\n",
    "# Your code for Exercise 3 is tested here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------ex1------------------------------------------\n",
      "-------------------------V.I.------------------------------\n",
      "[[7. 8. 9. 0. 9.]\n",
      " [6. 7. 8. 0. 8.]\n",
      " [5. 6. 7. 6. 7.]\n",
      " [4. 5. 6. 5. 6.]\n",
      " [3. 4. 5. 4. 5.]]\n",
      "-------------------------P.I.------------------------------\n",
      "[['e' 'e' 'e' 'n' 'w']\n",
      " ['n' 'n' 'n' 'n' 'n']\n",
      " ['n' 'n' 'n' 'e' 'n']\n",
      " ['n' 'n' 'n' 'n' 'n']\n",
      " ['n' 'n' 'n' 'n' 'n']]\n",
      "-----------------------------------------ex2------------------------------------------\n",
      "-------------------------V.I.------------------------------\n",
      "[[6.04169329 7.28756636 8.61359951 0.         8.69262311]\n",
      " [4.86185111 5.99087587 6.37082431 0.         6.46721593]\n",
      " [3.67550938 4.69621388 4.99441863 3.2189158  5.10250988]\n",
      " [2.48699534 3.40945989 3.66922967 2.64122933 3.78610115]\n",
      " [1.35979208 2.19733672 2.42878751 1.57272161 2.55202451]]\n",
      "-------------------------P.I.------------------------------\n",
      "[['e' 'e' 'e' 'n' 'w']\n",
      " ['n' 'n' 'n' 'n' 'n']\n",
      " ['n' 'n' 'n' 'e' 'n']\n",
      " ['n' 'n' 'n' 'e' 'n']\n",
      " ['n' 'n' 'n' 'n' 'n']]\n",
      "-----------------------------------------ex3------------------------------------------\n",
      "-------------------------V.I.------------------------------\n",
      "[[10.65103994 11.79603433 13.00848756  4.28547547 12.97048714]\n",
      " [11.1861353  12.32932372 12.5121464   0.         10.61568556]\n",
      " [12.28702748 13.59365638  4.99441863 12.41930416 11.19974428]\n",
      " [11.17522837 12.35165962 13.59092292 12.28122217 11.051285  ]\n",
      " [10.06409806 11.17488271 12.28045984 11.10816476  9.99389366]]\n",
      "-------------------------P.I.------------------------------\n",
      "[['e' 'e' 'e' 'n' 'w']\n",
      " ['e' 's' 's' 'n' 'n']\n",
      " ['e' 'e' 'n' 'w' 'w']\n",
      " ['e' 'n' 'n' 'w' 'w']\n",
      " ['n' 'n' 'n' 'n' 'w']]\n",
      "-----------------------------------------Completed------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print('-----------------------------------------ex1------------------------------------------')\n",
    "env1 = Gridworld_Det()\n",
    "%timeit                        \n",
    "print('-------------------------V.I.------------------------------')\n",
    "print(np.flip(env1.ValueIteration().reshape((5,5)),0))\n",
    "print('-------------------------P.I.------------------------------')\n",
    "print(np.flip(env1.PolicyIteration().reshape((5, 5)), 0))\n",
    "print('-----------------------------------------ex2------------------------------------------')\n",
    "env2 = Gridworld_Stoch()\n",
    "print('-------------------------V.I.------------------------------')\n",
    "print(np.flip(env2.ValueIteration().reshape((5,5)),0))\n",
    "print('-------------------------P.I.------------------------------')\n",
    "print(np.flip(env2.PolicyIteration().reshape((5, 5)), 0))\n",
    "print('-----------------------------------------ex3------------------------------------------')\n",
    "env3 = Gridworld_Stoc_gold()\n",
    "print('-------------------------V.I.------------------------------')\n",
    "print(np.flip(env3.ValueIteration().reshape((5,5)),0))\n",
    "print('-------------------------P.I.------------------------------')\n",
    "print(np.flip(env3.PolicyIteration().reshape((5, 5)), 0))\n",
    "print('-----------------------------------------Completed------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
